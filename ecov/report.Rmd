

```{r custom}
library(VariantAnnotation)
library(ggplot2)
library(dplyr)
library(pheatmap)
library(scales)
library(gridExtra)
library(gtools)
library(RColorBrewer)
library(knitr)
library(tidyr)
library(reshape)
library(rmarkdown)
# library(ggbio)
number_ticks <- function(n) {function(limits) pretty(limits, n)}
options(bitmapType = 'cairo')

path_results = "$path_results"
```

```{r create-report, echo=FALSE, eval=FALSE}
knitr::opts_chunk$set(tidy=TRUE, highlight=TRUE, dev="png",
                      cache=FALSE, highlight=TRUE, autodep=TRUE, warning=FALSE, error=FALSE,
                      eval=TRUE, fig.width= 9, echo=FALSE,
                      message=FALSE, prompt=TRUE, comment='', fig.cap='', bootstrap.show.code=FALSE)
render(file.path(path_results, "report", "report-ready.Rmd"))
```


## samples similarity

Samples with less than 5% of similarity. It should show samples more similar than a first degree relative. Or technical replicate with different library preparation or exactly the same sample if score is 0.

```{r qsignature, results='asis'}
sim = read.table(file.path(path_results, "qsignature.ma"))
names(sim) = c("sample1" , "sample2", "score")

kable(sim %>% filter(score<0.1) %>% arrange(score))

```


## quality control metrics

```{r load-metrics}
qc = read.table(file.path(path_results, "metrics", "metrics.tsv"),
                header=T, sep="\t", check.names=F,
                colClasses=list("sample"="character"))
rownames(qc) = qc$sample
qc$Mapped_reads_pct = as.numeric(gsub("%", "", qc$Mapped_reads_pct))
qc$Duplicates_pct = as.numeric(gsub("%", "", qc$Duplicates_pct))

qc_plus= read.table(file.path(path_results, "basic-bam", "flagstat.tsv"), header=T, sep= "\t", check.names = F)
qc$secondary = unlist(qc_plus[qc_plus$measure=="secondary",as.character(qc$sample)])
qc$singletons = unlist(qc_plus[qc_plus$measure=="singletons",as.character(qc$sample)])
```

### Mean quality along read

```{r read-qual}
qual = read.table(file.path(path_results, "fastqc", "Per_base_sequence_quality.tsv"), header=T, sep= "\t", check.names = F, colClasses=list("sample"="character"))
qual$sample = as.character(qual$sample)

ggplot(qual, aes(x=Base, y=Mean, group=sample)) +
    geom_line() +
    facet_wrap(~sample) +
    ylim(0,41) +
    scale_color_brewer(palette = "Set1") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5))+
    scale_x_discrete(breaks=number_ticks(10))
```

### Size distribution

```{r read-size}
qual = read.table(file.path(path_results, "fastqc", "Sequence_Length_Distribution.tsv"), header=T, sep= "\t", check.names = F, colClasses=list("sample"="character"))
qual = qual %>% group_by(sample) %>% mutate(total=sum(Count), pct=Count/total)

ggplot(qual , aes(x=Length, y=Count, group=sample)) +
    geom_line(size=2, alpha=0.5) +
    scale_color_brewer(palette = "Set1") +
    theme_bw() +
    facet_wrap(~sample) +
    labs(y="# of reads")+
    theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5)) +
    scale_x_discrete(breaks=number_ticks(5))
```

Length read with more than 25% of the total reads.

```{r r table-size, results='asis'}
kable(qual %>% filter(pct > 0.25) %>% select(Length, sample, pct) %>% spread(Length, pct), align="c", digits=2)

```

### Nucleotide content

```{r read-content}
qual = read.table(file.path(path_results, "fastqc", "Per_base_sequence_content.tsv"), header=T, sep= "\t", check.names = F, colClasses=list("sample"="character"))
qual$sample = as.character(qual$sample)

qual$Base = as.numeric(qual$Base)

dd = melt(qual, id.vars = c("Base", "sample"), variable_name = c("nt"))

ggplot(dd, aes(x=Base, y=value, group=sample)) +
        geom_line(size=2, alpha=0.5) +
        theme_bw() +
        ylab("% of nucleotides") +
        theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5)) +
        ylim(10,50) +
    facet_wrap(~nt)


```

Nt in position with > 30% or < 10% of total reads.

```{r table-content, results='asis'}
kable( melt(qual, id.vars=c("Base", "sample"), variable_name = "nt") %>%
           filter(value > 30 | value < 10) %>% filter(Base<20) %>%
           select(Base, sample, nt, value) %>%
           spread(Base, value),
       align="c", digits=2)


```


### Total reads

```{r total-reads}
ggplot(qc, aes(x=sample, y=Total_reads/1e6)) +
    geom_bar(stat = 'identity') +
    ylab("Million reads") +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Mapped reads

```{r mapped-reads}
ggplot(qc, aes(x=sample, y=Mapped_reads/Total_reads)) +
    geom_bar(stat = 'identity') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Offtarget reads

```{r off-reads}
ggplot(qc, aes(x=sample, y=offtarget/Mapped_reads)) +
    geom_bar(stat = 'identity') +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


### Basic metrics by samples

```{r table, results='asis'}
metrics = c("sample", "Total_reads" ,"Mapped_reads_pct", "Duplicates_pct",
            "singletons", "secondary", "offtarget",
            "%GC", "Sequence_length", "Median_insert_size")
qc$offtarget = qc$offtarget/qc$Total_reads
qc$secondary = qc$secondary/qc$Total_reads
qc$singletons = qc$singletons/qc$Total_reads
print(kable(qc[, metrics], align="c", digits=2))

```

### Variant metrics by samples

```{r table-variants, results='asis'}
qc$ratio_het_hom = qc$Variations_heterozygous/qc$Variations_homozygous
metrics = c("sample", "Variations_total", "Variations_in_dbSNP_pct",
            "Variations_heterozygous", "Variations_homozygous", "ratio_het_hom", "Transition/Transversion")
print(kable(qc[, metrics], align="c", digits=2))

```

## coverage

### variants coverage

figure cut at 100 at the x axis.

```{r variants-coverage}
fns = list.files(file.path(path_results, "cg"), full.names = TRUE, pattern = "cg-depth-parse.tsv")
tab = data.frame()
for (fn in fns){
    dt = read.table(fn, header=T,sep="\t")
    dt = dt %>% filter(!grepl("[::.::]",depth))
    dt[,2] = as.numeric(dt[,2])
    q = quantile(dt[,2],c(0,.10,.25,.50,.75,.90,1))
    labels=factor(rev(names(q)),levels=c("0%","10%","25%","50%","75%","90%","100%"))
    dt = data.frame(variants_pct=labels, depth=q, sample=dt$sample[1])
    tab = rbind(tab, dt)
}


ggplot(tab, aes(x=depth, y=variants_pct, group=sample)) +
    geom_line(size=2, alpha=.5)+
    theme_bw() +
    xlim(0,100) +
    labs(list(x="# of reads", y="% variants with more than X reads", title="variants coverage"))
```

### variants coverage vs CG content

```{r variants-coverage-gc, fig.width=15, fig.height=15}
rf <- colorRampPalette(rev(brewer.pal(11,'Spectral')))
r <- rf(32)
list_p = list()
fns = list.files(file.path(path_results, "cg"), full.names = TRUE, pattern = "cg-depth-parse.tsv")
for (fn in fns){
    dt = read.table(fn, header=T,sep="\t")

    dt = dt %>% filter(!grepl("[::.::]",depth))
    dt[,2] = as.numeric(dt[,2])
    sample = dt$sample[1]
    p = ggplot(dt, aes(CG, depth)) +
        stat_bin2d() +
        ylab("# of reads") +
        scale_fill_gradientn(guide = FALSE,colours=r) +
        theme_bw() +
        ylim(0,700) +
        ggtitle(sample)


    list_p[[as.character(sample)]]=p

}

do.call(grid.arrange, list_p)
```


```{r total-coverage-load}
get_quantile_cov = function(path){
    cov_tab = data.frame()
    co = c('q10', 'q25', 'q50')
    for (fn in list.files(path, pattern = "_cov.tsv", full.names = TRUE)){
            d = read.table(fn, header=T, sep="\t")
            for (i in 1:3){
                # q = quantile(d[,i],c(.10,.25,.50,.75,.90))
                q = quantile(d[,i],c(0.01,.10,.25,.50,.75,.90,.99))
                labels=factor(rev(names(q)),levels=c("1%","10%","25%","50%","75%","90%","99%"))

                s = gsub("-ready","",d[1,6])
                t = data.frame(quantiles=q*100, type=labels, sample=s, min_reads=co[i])
                cov_tab = rbind(cov_tab, t)
            }

    }
    cov_tab
}


get_total_cov = function(path){
    cov_tab = data.frame()
    for (fn in list.files(path, pattern = "_cov_total.tsv", full.names = TRUE)){
            d = read.table(fn, header=T, sep="\t")
            pct = 100 - cumsum(d[,2])/d[,3] * 100
            s = gsub("-ready","",d[1,4])
            t = data.frame(depth=d[,1], bases=pct, sample=s)
            cov_tab = rbind(cov_tab, t)
    }
    cov_tab
}


make_total_cov_plots = function(cov_tab){
    p =ggplot(cov_tab, aes(y=bases, x=depth, group=sample)) +
        geom_line(size=2, alpha=.5)+
        theme_bw()+
        labs(list(y="% of bed file > depth", x="# of reads"))
    print(p)
}

make_quantile_plots = function(cov_tab){
    p1 = ggplot(cov_tab %>% filter(min_reads=='q10'), aes(x=type, y=quantiles,  group=sample)) +
    geom_line(size=2, alpha=.5)+
    theme_bw()+
    labs(list(x="% of target regions with\nmore than X bases covered", y="% of nt covered\ninside the target", title="considered covered when nt has >10 reads"))

    p2 = ggplot(cov_tab %>% filter(min_reads=='q25'), aes(x=type, y=quantiles,  group=sample)) +
    geom_line(size=2, alpha=.5)+
    theme_bw()+
    labs(list(x="% of target regions with\nmore than X bases covered", y="% of nt covered\ninside the target", title="considered covered when nt has >25 reads"))

    p3 = ggplot(cov_tab %>% filter(min_reads=='q50'), aes(x=type, y=quantiles, group=sample)) +
    geom_line(size=2, alpha=.5)+
    theme_bw()+
    labs(list(x="% of target regions with\nmore than X bases covered", y="% of nt covered\ninside the target", title="considered covered when nt has >50 reads"))

    grid.arrange(p1, p2, p3, ncol=1)
}

```

### coverage distribution (total)


```{r cov-total-fig, fig.height=6, fig.width=11, cache=TRUE}
cov_tab = get_total_cov(file.path(path_results, "coverage"))
make_total_cov_plots(cov_tab)
```

### coverage distribution (completeness)

```{r completeness-fig, fig.height=12, fig.width=12, cache=TRUE}
cov_tab = get_quantile_cov(file.path(path_results, "coverage"))
make_quantile_plots(cov_tab)
```

Values at 90% of regions

```{r table-completeness, results='asis'}
kable(cov_tab %>% filter(type == "90%") %>%
          spread(min_reads, quantiles) %>%
          select(targets_pct=type, sample, min_10=q10, min_25=q25, min_50=q50), align="c", digits=2)

```


### coverage uniformity

Sampling 1000 regions from the target bed file.

```{r cov-uniformity-load}
cov_tab = data.frame()

get_bias_cov = function(path){
    for (fn in list.files(path, pattern = "_bias.tsv", full.names = TRUE)){
            d = read.table(fn, header=T, sep="\t")

            cv = d[,"std"]/d[,"mean"]
            bias = (d[,"ntdow"] + d[,"ntup"])/d[,"size"] * 100
            s = as.character(gsub("-ready","",d[1,"sample"]))
            t = data.frame(bias=bias, cv=cv, mean=d[,"mean"], sample=s)
            cov_tab = rbind(cov_tab, t)


    }
    cov_tab
}

make_bias_plot = function(cov_tab){
    p1 = ggplot(cov_tab, aes(x=log2(mean), y=cv)) +
    geom_point(alpha=0.5) +
    scale_color_brewer(palette = "Set1") +
    labs(list(y="coefficient of variation",x="log2(mean coverage)")) +
    theme_bw() +
        ggtitle("coverage variation for each target region")

    p2 = ggplot(cov_tab, aes( x=sample,y=bias)) +
    geom_jitter(alpha=0.5) +
    scale_color_brewer(palette = "Set1") +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
    labs(list(y="% of nt with mean-2SD > coverage > mean+2SD ")) +
        ggtitle("% of nucleotides with extreme\ncoverage inside target regions")
    # grid.arrange(p1, p2, ncol=1)
    print(p2)
}
```

```{r cov-uniformity-load-agilent, fig.height=12, fig.width=12, cache=TRUE, eval=FALSE, echo=FALSE}
bias_tab = get_bias_cov(file.path(path_results,"bias"))
make_bias_plot(bias_tab)
```
